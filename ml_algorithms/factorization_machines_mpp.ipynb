{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machines:\n",
    "### This notebook illustrates a distributed mplementation of Steffen Rendle's Factorization Machines on MPP Databases such as Greenplum and HAWQ.\n",
    "\n",
    "### Reference: Factorization Machines By Steffen Rendle\n",
    "### http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create a database connection to Greenplum or HAWQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run '../database_connectivity_setup.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create a new schema for Factorization Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"create schema ml;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two parallel gradient descent implementations are provided: 1) mini batch stochastic gradient descent (batch-sgd) and 2) regular batch gradient descent. The batch-sgd algorithm assumes the training data has been distributed randomly across multiple segments. The weights are updated per segment based on mini-batches of data in each segment and the weights are averaged in the end at the master node. The approach here is similar to Algorithm 3 in http://www.research.rutgers.edu/~lihong/pub/Zinkevich11Parallelized.pdf. The only difference being while the data per segment is randomly shuffled in each epoch for updating the parameters per mini-batch on that segment, the random distribution of the entire data across all segments happens only at the beginning.\n",
    "\n",
    "### Let us first create the batch-sgd update function in PL/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop function if exists ml.fm_sgd_update (\n",
    "            text[], \n",
    "            float8[], \n",
    "            float8[], \n",
    "            int, \n",
    "            int, \n",
    "            float8, \n",
    "            float8, \n",
    "            float8, \n",
    "            float8,\n",
    "            float8,\n",
    "            float8,\n",
    "            varchar\n",
    "        );\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create or replace function ml.fm_sgd_update (\n",
    "        xarr text[],\n",
    "        yarr float8[],\n",
    "        w float8[],\n",
    "        n int,\n",
    "        k int,\n",
    "        miny float8,\n",
    "        maxy float8,\n",
    "        lambda0 float8,\n",
    "        lambda1 float8,\n",
    "        lambda2 float8,\n",
    "        learning_rate float8,\n",
    "        rseed varchar\n",
    "    )\n",
    "    returns float8[]\n",
    "    AS\n",
    "    $$\n",
    "        import numpy as np\n",
    "        \n",
    "        def update_params_batch(\n",
    "            x,\n",
    "            y,\n",
    "            w_t,\n",
    "            n,\n",
    "            k,\n",
    "            miny,\n",
    "            maxy,\n",
    "            lambda0,\n",
    "            lambda1,\n",
    "            lambda2,\n",
    "            learning_rate,\n",
    "            b\n",
    "        ):\n",
    "            w0 = w_t[0]\n",
    "            w  = np.reshape(np.array(w_t[1:n+1]), (1,n))\n",
    "            v  = np.matrix(np.reshape(w_t[n+1:],(n,k)))\n",
    "\n",
    "            y_hat = predict(x,w0,w,v)\n",
    "            y_hat = np.minimum(maxy, y_hat)\n",
    "            y_hat = np.maximum(miny, y_hat)\n",
    "            term1 = 2*(y_hat - y[:,np.newaxis])\n",
    "            gradw0 = np.sum(term1, axis = 0) + lambda0*w0\n",
    "            if(gradw0.shape[0] <> 1 or gradw0.shape[1] <> 1):\n",
    "                raise ValueError('Gradient w0 has more than 1 element')\n",
    "            gradw = np.sum(np.multiply(term1,x), axis = 0) + lambda1*w\n",
    "            if(gradw.shape[1] <> x.shape[1]):\n",
    "                raise ValueError('Gradient w has incorrect number of elements')   \n",
    "            gradv = np.zeros(v.shape)\n",
    "            d = x.shape[0]\n",
    "            instances = range(0,d)\n",
    "            for dd in instances:\n",
    "                xd = x[dd,:]\n",
    "                xdv = xd*v\n",
    "                gradv += term1[dd][0,0] * (\n",
    "                            np.multiply(xd.T,np.repeat(xdv,xd.shape[1],axis=0)) -\n",
    "                            np.multiply(v,np.square(xd.T))\n",
    "                        ) \n",
    "            gradv += lambda2*v\n",
    "\n",
    "            w0 -= (learning_rate/b*1.0)*gradw0\n",
    "            w  -= (learning_rate/b*1.0)*gradw\n",
    "            v  -= (learning_rate/b*1.0)*gradv\n",
    "            \n",
    "            return [w0[0,0]] + w[0,:].tolist() + np.array(np.reshape(v,(1,n*k))).tolist()[0]\n",
    "            \n",
    "        def update_params(\n",
    "            x,\n",
    "            y,\n",
    "            w_t,\n",
    "            n,\n",
    "            k,\n",
    "            miny,\n",
    "            maxy,\n",
    "            lambda0,\n",
    "            lambda1,\n",
    "            lambda2,\n",
    "            learning_rate\n",
    "        ):\n",
    "            w0 = w_t[0]\n",
    "            w  = np.reshape(np.array(w_t[1:n+1]),(1,n))\n",
    "            v  = np.matrix(np.reshape(w_t[n+1:],(n,k)))\n",
    "\n",
    "            y_hat = predict(x,w0,w,v)\n",
    "            y_hat = np.minimum(maxy, y_hat)\n",
    "            y_hat = np.maximum(miny, y_hat)\n",
    "            term1 = 2*(y_hat - y)\n",
    "            gradw0 = term1 + lambda0*w0\n",
    "            if(gradw0.shape[0] <> 1 or gradw0.shape[1] <> 1):\n",
    "                raise ValueError('Gradient w0 has more than 1 element')\n",
    "            gradw = np.multiply(term1,x) + lambda1*w\n",
    "            if(gradw.shape[1] <> x.shape[1]):\n",
    "                raise ValueError('Gradient w has incorrect number of elements')   \n",
    "            gradv = np.zeros(v.shape)\n",
    "            xdv = x*v\n",
    "            gradv += term1[0,0] * (\n",
    "                        np.multiply(x.T,np.repeat(xdv,x.shape[1],axis=0)) -\n",
    "                        np.multiply(v,np.square(x.T))\n",
    "                    ) \n",
    "            gradv += lambda2*v\n",
    "            w0 -= learning_rate*gradw0\n",
    "            w  -= learning_rate*gradw\n",
    "            v  -= learning_rate*gradv\n",
    "            return [w0[0,0]] + w[0,:].tolist() + np.array(np.reshape(v,(1,n*k))).tolist()[0]\n",
    "            \n",
    "        def predict(x, w0, w, v):\n",
    "            return (w0 + \n",
    "                x*w.T + \n",
    "                0.5*(np.sum(np.square(x*v) - (np.square(x)*np.square(v)),axis=1))\n",
    "               )\n",
    "        \n",
    "        arr = ','.join(xarr)\n",
    "        x = np.matrix(arr,dtype=float)\n",
    "        x = x.reshape(x.shape[1]/n, n)\n",
    "        d = x.shape[0]\n",
    "        minibatch_size = 300\n",
    "        indx = range(0,d)\n",
    "        random_seed = None\n",
    "        if (rseed != 'None'):\n",
    "            random_seed = int('rseed')\n",
    "        if (random_seed is not None):\n",
    "            np.random.seed(random_seed)\n",
    "        w_t = np.array(w)\n",
    "        y = np.array(yarr,dtype=float)\n",
    "        if (d >= 600):\n",
    "            shuffledindx = np.random.permutation(indx)\n",
    "            start = 0;\n",
    "            end = minibatch_size\n",
    "            while (start < d):\n",
    "                xbatch = x[shuffledindx[start:end],:]\n",
    "                ybatch = y[shuffledindx[start:end]]\n",
    "                w_t = update_params_batch(\n",
    "                    xbatch,\n",
    "                    ybatch,\n",
    "                    w_t,\n",
    "                    n,\n",
    "                    k,\n",
    "                    miny,\n",
    "                    maxy,\n",
    "                    lambda0,\n",
    "                    lambda1,\n",
    "                    lambda2,\n",
    "                    learning_rate,\n",
    "                    xbatch.shape[0]\n",
    "                )\n",
    "                start += minibatch_size\n",
    "                end += minibatch_size\n",
    "                if (end > d):\n",
    "                    end = d\n",
    "        else:\n",
    "            shuffledindx = np.random.permutation(indx)\n",
    "            for i in shuffledindx:\n",
    "                xd = x[i,:]\n",
    "                yd = y[i]\n",
    "                w_t = update_params(\n",
    "                    xd,\n",
    "                    yd,\n",
    "                    w_t,\n",
    "                    n,\n",
    "                    k,\n",
    "                    miny,\n",
    "                    maxy,\n",
    "                    lambda0,\n",
    "                    lambda1,\n",
    "                    lambda2,\n",
    "                    learning_rate\n",
    "                )\n",
    "        return w_t\n",
    "    $$language plpythonu;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us next create the regular batch gradient function in PL/Python. Here, we compute only the gradient of the loss function in a parallel fashion. The gradients are added up in the master node along with the gradient of the regularization component and the weights are updated in the master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop function if exists ml.fm_loss_gradient(\n",
    "            text[], \n",
    "            float8[], \n",
    "            float8[], \n",
    "            int, \n",
    "            int, \n",
    "            float8, \n",
    "            float8,\n",
    "            float8\n",
    "        );\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create or replace function ml.fm_loss_gradient (\n",
    "        xarr text[],\n",
    "        yarr float8[],\n",
    "        w float8[],\n",
    "        n int,\n",
    "        k int,\n",
    "        miny float8,\n",
    "        maxy float8,\n",
    "        trnsize float8\n",
    "    )\n",
    "    returns float8[]\n",
    "    AS\n",
    "    $$\n",
    "        import numpy as np\n",
    "        \n",
    "        def compute_loss_gradient(\n",
    "            x,\n",
    "            y,\n",
    "            w_t,\n",
    "            n,\n",
    "            k,\n",
    "            miny,\n",
    "            maxy,\n",
    "            trnsize\n",
    "        ):\n",
    "            d = x.shape[0]\n",
    "            if (d == 1):\n",
    "                loss_grad = compute_loss_gradient_single(x,y,w_t,n,k,miny,maxy)\n",
    "                return loss_grad\n",
    "                \n",
    "            w0 = w_t[0]\n",
    "            w  = np.reshape(np.array(w_t[1:n+1]),(1,n))\n",
    "            v  = np.matrix(np.reshape(w_t[n+1:],(n,k)))\n",
    "\n",
    "            y_hat = predict(x,w0,w,v)\n",
    "            y_hat = np.minimum(maxy, y_hat)\n",
    "            y_hat = np.maximum(miny, y_hat)\n",
    "            term1 = (1.0/trnsize)*(y_hat - y[:,np.newaxis])\n",
    "            gradw0 = np.sum(term1, axis = 0)\n",
    "            if(gradw0.shape[0] <> 1 or gradw0.shape[1] <> 1):\n",
    "                raise ValueError('Gradient w0 has more than 1 element')\n",
    "            gradw = np.array(np.sum(np.multiply(term1,x), axis = 0))\n",
    "            if(gradw.shape[1] <> x.shape[1]):\n",
    "                raise ValueError('Gradient w has incorrect number of elements')   \n",
    "            gradv = np.matrix(np.zeros(v.shape))\n",
    "            instances = range(0,d)\n",
    "            for dd in instances:\n",
    "                xd = x[dd,:]\n",
    "                xdv = xd*v\n",
    "                gradv += term1[dd][0,0] * (\n",
    "                            np.multiply(xd.T,np.repeat(xdv,xd.shape[1],axis=0)) -\n",
    "                            np.multiply(v,np.square(xd.T))\n",
    "                        ) \n",
    "\n",
    "            loss_grad = [gradw0[0,0]] + gradw[0,:].tolist() + np.array(np.reshape(gradv,(1,n*k))).tolist()[0]\n",
    "            return loss_grad\n",
    "            \n",
    "        def compute_loss_gradient_single(\n",
    "            x,\n",
    "            y,\n",
    "            w_t,\n",
    "            n,\n",
    "            k,\n",
    "            miny,\n",
    "            maxy,\n",
    "            trnsize\n",
    "        ):\n",
    "            w0 = w_t[0]\n",
    "            w  = np.reshape(np.array(w_t[1:n+1]),(1,n))\n",
    "            v  = np.matrix(np.reshape(w_t[n+1:],(n,k)))\n",
    "            \n",
    "            y_hat = predict(x,w0,w,v)\n",
    "            y_hat = np.minimum(maxy, y_hat)\n",
    "            y_hat = np.maximum(miny, y_hat)\n",
    "            term1 = (1.0/trnsize)*(y_hat - y)\n",
    "            gradw0 = term1\n",
    "            if(gradw0.shape[0] <> 1 or gradw0.shape[1] <> 1):\n",
    "                raise ValueError('Gradient w0 has more than 1 element')\n",
    "            gradw = np.array(np.multiply(term1,x))\n",
    "            if(gradw.shape[1] <> x.shape[1]):\n",
    "                raise ValueError('Gradient w has incorrect number of elements')   \n",
    "            gradv = np.matrix(np.zeros(v.shape))\n",
    "            xdv = x*v\n",
    "            gradv += term1[0,0] * (\n",
    "                        np.multiply(x.T,np.repeat(xdv,x.shape[1],axis=0)) -\n",
    "                        np.multiply(v,np.square(x.T))\n",
    "                    ) \n",
    "            loss_grad = [gradw0[0,0]] + gradw[0,:].tolist() + np.array(np.reshape(gradv,(1,n*k))).tolist()[0]\n",
    "            return loss_grad\n",
    "            \n",
    "        def predict(x, w0, w, v):\n",
    "            return (w0 + \n",
    "                x*w.T + \n",
    "                0.5*(np.sum(np.square(x*v) - (np.square(x)*np.square(v)),axis=1))\n",
    "               )\n",
    "        \n",
    "        arr = ','.join(xarr)\n",
    "        x = np.matrix(arr,dtype=float)\n",
    "        x = x.reshape(x.shape[1]/n, n)\n",
    "        w_t = np.array(w,dtype=float)\n",
    "        y = np.array(yarr,dtype=float)\n",
    "        loss_grad = compute_loss_gradient(x,y,w_t,n,k,miny,maxy,trnsize)\n",
    "        return loss_grad\n",
    "    $$language plpythonu;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create the driver function for calling the per-segment batch or batch-sgd PL/Python UDFs. This will be the function that the end user will call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Driver function\n",
    "sql = \"\"\"drop function if exists ml.fm_fit(varchar,varchar,varchar,varchar,varchar,varchar);\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "# Parameters of the fm_fit function include:\n",
    "# 1. source_table - table containing the feature vectors and dependent variable as two separate columns\n",
    "# 2. id_col_name - an id column for each row\n",
    "# 3. x_col_name - features\n",
    "# 4. y_col_name - dependent variable\n",
    "# 5. parameters - learning parameters: regularizers (lambda0, lambda1, lambda2), number of iterations (n_iter), learning rate\n",
    "# (learning_rate), factorization hyper-parameter (k), random seed (integer for batch-sgd), optim (batch or batch-sgd)\n",
    "# out_table - name of the table where the model will be stored\n",
    "\n",
    "sql = \"\"\"\n",
    "    create or replace function ml.fm_fit (\n",
    "        src_table varchar,\n",
    "        id_col_name varchar,\n",
    "        x_col_name varchar,\n",
    "        y_col_name varchar,\n",
    "        parameters varchar,\n",
    "        out_table varchar\n",
    "    )\n",
    "    returns text\n",
    "    AS\n",
    "    $$\n",
    "        import numpy as np\n",
    "        \n",
    "        # Initialize the parameters to their default values\n",
    "        lambda0 = 1\n",
    "        lambda1 = 1\n",
    "        lambda2 = 1\n",
    "        n_iter = 100\n",
    "        learning_rate = 1\n",
    "        k = 50\n",
    "        random_seed = 'None'\n",
    "        optim = 'batch-sgd'\n",
    "        params = {}\n",
    "\n",
    "        for item in parameters.split(','):\n",
    "            params[item.split('=')[0].strip()] = item.split('=')[1].strip()\n",
    "        \n",
    "        if params.has_key('lambda0'):\n",
    "            lambda0 = float(params['lambda0'])\n",
    "        if params.has_key('lambda1'):\n",
    "            lambda1 = float(params['lambda1'])\n",
    "        if params.has_key('lambda2'):\n",
    "            lambda2 = float(params['lambda2'])\n",
    "        if params.has_key('n_iter'):\n",
    "            n_iter = int(params['n_iter'])\n",
    "        if params.has_key('learning_rate'):\n",
    "            learning_rate = float(params['learning_rate'])\n",
    "        if params.has_key('k'):\n",
    "            k = int(params['k'])\n",
    "        if params.has_key('random_seed'):\n",
    "            random_seed = int(params['random_seed'])\n",
    "        if params.has_key('optim'):\n",
    "            optim = params['optim']\n",
    "            plpy.info('optimization selected: ' + optim)\n",
    "        \n",
    "        sql = '''\n",
    "            select array_upper({x_col_name},1) as n from {src_table} limit 1;\n",
    "        '''.format(x_col_name=x_col_name,src_table=src_table)\n",
    "        rv = plpy.execute(sql)\n",
    "        n = rv[0]['n']\n",
    "        \n",
    "        sql = '''\n",
    "            select count(distinct {id_col_name}) as trnsize from {src_table} limit 1;\n",
    "        '''.format(id_col_name=id_col_name,src_table=src_table)\n",
    "        rv = plpy.execute(sql)\n",
    "        trnsize = rv[0]['trnsize']\n",
    "        \n",
    "        sql = '''\n",
    "            select min({y_col_name})::float8 as miny, max({y_col_name})::float8 as maxy from {src_table} limit 1;\n",
    "        '''.format(y_col_name=y_col_name,src_table=src_table)\n",
    "        rv = plpy.execute(sql)\n",
    "        miny = rv[0]['miny']\n",
    "        maxy = rv[0]['maxy']\n",
    "        \n",
    "        w0 = 0\n",
    "        w = np.zeros([1,n])\n",
    "        v = np.random.normal(scale=0.1,size=(n,k))\n",
    "        \n",
    "        w_t = [w0] + w[0,:].tolist() + np.array(np.reshape(v,(1,n*k))).tolist()[0]\n",
    "        \n",
    "        for i in range(0,n_iter):\n",
    "            plpy.info(\"Epoch number: \" + str(i))\n",
    "            \n",
    "            if optim == 'batch':\n",
    "                sql = \\\"\"\"\n",
    "                    select\n",
    "                        ml.fm_loss_gradient (\n",
    "                            xarr,\n",
    "                            yarr,\n",
    "                            array{w_t},\n",
    "                            {n},\n",
    "                            {k},\n",
    "                            {miny},\n",
    "                            {maxy},\n",
    "                            {trnsize}\n",
    "                        ) as loss_grad\n",
    "                    from (\n",
    "                        select\n",
    "                            gp_segment_id,\n",
    "                            array_agg(array_to_string({x_col_name},',') order by {id_col_name}) as xarr,\n",
    "                            array_agg({y_col_name} order by {id_col_name}) as yarr\n",
    "                        from            \n",
    "                            {src_table}\n",
    "                        group by 1\n",
    "                    )q1;\n",
    "                \\\"\"\".format(\n",
    "                        src_table=src_table,\n",
    "                        x_col_name=x_col_name,\n",
    "                        y_col_name=y_col_name,\n",
    "                        id_col_name=id_col_name,\n",
    "                        w_t=w_t,\n",
    "                        n=n,\n",
    "                        k=k,\n",
    "                        miny=miny,\n",
    "                        maxy=maxy,\n",
    "                        trnsize=trnsize\n",
    "                    )\n",
    "                rv = plpy.execute(sql)\n",
    "                nrows = len(rv)\n",
    "                loss_grad = np.zeros(shape=(1,n+1+(n*k)))\n",
    "                for r in range(0,nrows):\n",
    "                    loss_grad += np.array(rv[r]['loss_grad'])\n",
    "\n",
    "                loss_grad = loss_grad[0]\n",
    "                loss_grad_w0 = loss_grad[0]\n",
    "                loss_grad_w  = np.reshape(np.array(loss_grad[1:n+1]),(1,n))\n",
    "                loss_grad_v  = np.matrix(np.reshape(loss_grad[n+1:],(n,k)))\n",
    "                gradw0 = loss_grad_w0 + lambda0*w0\n",
    "                gradw  = loss_grad_w + lambda1*w\n",
    "                gradv  = loss_grad_v + lambda2*v\n",
    "                w0 -= (learning_rate)*gradw0\n",
    "                w  -= (learning_rate)*gradw\n",
    "                v  -= (learning_rate)*gradv\n",
    "                w_t = [w0] + w[0,:].tolist() + np.array(np.reshape(v,(1,n*k))).tolist()[0]\n",
    "            elif optim == 'batch-sgd':\n",
    "                sql = \\\"\"\"\n",
    "                    select\n",
    "                        ml.fm_sgd_update (\n",
    "                            xarr,\n",
    "                            yarr,\n",
    "                            array{w_t},\n",
    "                            {n},\n",
    "                            {k},\n",
    "                            {miny},\n",
    "                            {maxy},\n",
    "                            {lambda0},\n",
    "                            {lambda1},\n",
    "                            {lambda2},\n",
    "                            {learning_rate},\n",
    "                            '{random_seed}'\n",
    "                        ) as w_curr\n",
    "                    from (\n",
    "                        select\n",
    "                            gp_segment_id,\n",
    "                            array_agg(array_to_string({x_col_name},',') order by {id_col_name}) as xarr,\n",
    "                            array_agg({y_col_name} order by {id_col_name}) as yarr\n",
    "                        from            \n",
    "                            {src_table}\n",
    "                        group by 1\n",
    "                    )q1;\n",
    "                \\\"\"\".format(\n",
    "                        src_table=src_table,\n",
    "                        x_col_name=x_col_name,\n",
    "                        y_col_name=y_col_name,\n",
    "                        id_col_name=id_col_name,\n",
    "                        w_t=w_t,\n",
    "                        n=n,\n",
    "                        k=k,\n",
    "                        miny=miny,\n",
    "                        maxy=maxy,\n",
    "                        lambda0=lambda0,\n",
    "                        lambda1=lambda1,\n",
    "                        lambda2=lambda2,\n",
    "                        learning_rate=learning_rate,\n",
    "                        random_seed=random_seed\n",
    "                    )\n",
    "                rv = plpy.execute(sql)\n",
    "                nrows = len(rv)\n",
    "                w_t = np.zeros(shape=(1,n+1+(n*k)))\n",
    "                for r in range(0,nrows):\n",
    "                    w_t += np.array(rv[r]['w_curr'])\n",
    "                w_t[0,:] = w_t[0,:]/nrows\n",
    "                w_t = w_t[0,:].tolist()\n",
    "            \n",
    "        sql = \\\"\"\"\n",
    "            create table {out_table} (\n",
    "                n int4,\n",
    "                k int4,\n",
    "                miny float8,\n",
    "                maxy float8,\n",
    "                w float8[]\n",
    "            ) distributed randomly;\n",
    "        \\\"\"\".format(out_table=out_table)\n",
    "        plpy.execute(sql)\n",
    "        \n",
    "        sql = \\\"\"\"\n",
    "            insert into {out_table} values ({n},{k},{miny},{maxy},array{w});\n",
    "        \\\"\"\".format(out_table=out_table,n=n,k=k,miny=miny,maxy=maxy,w=w_t)\n",
    "        plpy.execute(sql)\n",
    "        \n",
    "        return \"Table {out_table} created.\".format(out_table=out_table)\n",
    "    $$language plpythonu;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us next define the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    create or replace function ml.fm_predict (\n",
    "        xarr float8[],\n",
    "        n int,\n",
    "        k int,\n",
    "        w_t float8[],\n",
    "        miny float8,\n",
    "        maxy float8\n",
    "    )\n",
    "    returns float8\n",
    "    AS\n",
    "    $$\n",
    "        import numpy as np\n",
    "        x = np.matrix(xarr, dtype=float)\n",
    "        w0 = w_t[0]\n",
    "        w  = np.reshape(np.array(w_t[1:n+1]),(1,n))\n",
    "        v  = np.matrix(np.reshape(w_t[n+1:],(n,k)))\n",
    "        y_hat = (w0 + \n",
    "                x*w.T + \n",
    "                0.5*(np.sum(np.square(x*v) - (np.square(x)*np.square(v)),axis=1))\n",
    "               )\n",
    "\n",
    "        y_hat = np.minimum(maxy, y_hat)\n",
    "        y_hat = np.maximum(miny, y_hat)\n",
    "        return y_hat[0,0]       \n",
    "    $$language plpythonu;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with MovieLens 100-K Ratings Data. The dataset is a popular dataset for recommender systems and can be downloaded here: http://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "### We will load the data into tables in GPDB and HAWQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop table if exists ml.fm_train_data;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"drop table if exists ml.fm_test_data;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create table ml.fm_train_data (\n",
    "        user_id int4,\n",
    "        item_id int4,\n",
    "        rating int4,\n",
    "        ts double precision\n",
    "    ) distributed randomly;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create table ml.fm_test_data (\n",
    "        user_id int4,\n",
    "        item_id int4,\n",
    "        rating int4,\n",
    "        ts double precision\n",
    "    ) distributed randomly;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "#COPY ml.fm_train_data FROM '/home/gmurlidhar/data/movielens/ua.base' CSV DELIMITER E'\\t' NULL '';\n",
    "#COPY ml.fm_test_data FROM '/home/gmurlidhar/data/movielens/ua.test' CSV DELIMITER E'\\t' NULL '';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL/Python UDF to create a feature vector from the [user_id, item_id] tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop function if exists ml.create_fm_feat_vec(int4,int4,int4[],int4[]) cascade;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    CREATE OR REPLACE FUNCTION ml.create_fm_feat_vec(\n",
    "        user_id int4,\n",
    "        item_id int4,\n",
    "        all_users int4[],\n",
    "        all_items int4[]\n",
    "    )\n",
    "    RETURNS int4[]\n",
    "    AS\n",
    "    $$\n",
    "        import numpy as np\n",
    "        useridx = {}\n",
    "        itemidx = {}\n",
    "        numusers = len(all_users)\n",
    "        numitems = len(all_items)\n",
    "        if 'useridx' not in SD:\n",
    "            for i in range(0,numusers):\n",
    "                useridx[all_users[i]] = i\n",
    "            SD['useridx'] = useridx\n",
    "        else:\n",
    "            useridx = SD['useridx']\n",
    "        \n",
    "        if 'itemidx' not in SD:\n",
    "            for i in range(0,numitems):\n",
    "                itemidx[all_items[i]] = i\n",
    "            SD['itemidx'] = itemidx\n",
    "        else:\n",
    "            itemidx = SD['itemidx']\n",
    "            \n",
    "        x = [0 for i in range(0,numusers+numitems)]\n",
    "        \n",
    "        if useridx.has_key(user_id):\n",
    "            x[useridx[user_id]] = 1\n",
    "        if itemidx.has_key(item_id):\n",
    "            x[numusers+itemidx[item_id]] = 1\n",
    "        return x\n",
    "    $$ LANGUAGE PLPYTHONU;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop table if exists ml.fm_train_table;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create table ml.fm_train_table as (\n",
    "        select\n",
    "            row_number() over() as id,\n",
    "            ml.create_fm_feat_vec (\n",
    "                t1.user_id,\n",
    "                t1.item_id,\n",
    "                t2.all_users::int4[],\n",
    "                t3.all_items::int4[]\n",
    "            ) as x,\n",
    "            t1.rating as y\n",
    "        from\n",
    "            ml.fm_train_data t1,\n",
    "            (\n",
    "                select\n",
    "                    array_agg(user_id order by user_id) as all_users\n",
    "                from (\n",
    "                    select\n",
    "                        user_id\n",
    "                    from\n",
    "                        ml.fm_train_data\n",
    "                    group by 1\n",
    "                )t21\n",
    "            ) t2,\n",
    "            (\n",
    "                select\n",
    "                    array_agg(item_id order by item_id) as all_items\n",
    "                from (\n",
    "                    select\n",
    "                        item_id\n",
    "                    from\n",
    "                        ml.fm_train_data\n",
    "                    group by 1\n",
    "                )t31\n",
    "            ) t3\n",
    "    ) distributed randomly;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_rows\n",
      "0     90570\n",
      "   num_cols\n",
      "0      2623\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"select count(*) as num_rows from ml.fm_train_table;\"\"\"\n",
    "print psql.read_sql(sql,conn)\n",
    "\n",
    "sql = \"\"\"select array_upper(x,1) as num_cols from ml.fm_train_table group by 1;\"\"\"\n",
    "print psql.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the fm_fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    select \n",
    "        ml.fm_fit (\n",
    "            'ml.fm_train_table',\n",
    "            'id',\n",
    "            'x',\n",
    "            'y',\n",
    "            'lambda0 = 0.01, lambda1 = 0.01, lambda2 = 0.01, k = 10, n_iter = 100, learning_rate = 0.5, optim=batch-sgd',\n",
    "            'ml.fm_mdl_table'\n",
    "        ); \n",
    "\"\"\"\n",
    "df = psql.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the testing table to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop table if exists ml.fm_test_table;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create table ml.fm_test_table as (\n",
    "        select\n",
    "            row_number() over() as id,\n",
    "            ml.create_fm_feat_vec (\n",
    "                t1.user_id,\n",
    "                t1.item_id,\n",
    "                t2.all_users::int4[],\n",
    "                t3.all_items::int4[]\n",
    "            )::float8[] as x,\n",
    "            t1.rating::float8 as y\n",
    "        from\n",
    "            ml.fm_test_data t1,\n",
    "            (\n",
    "                select\n",
    "                    array_agg(user_id order by user_id) as all_users\n",
    "                from (\n",
    "                    select\n",
    "                        user_id\n",
    "                    from\n",
    "                        ml.fm_train_data\n",
    "                    group by 1\n",
    "                )t21\n",
    "            ) t2,\n",
    "            (\n",
    "                select\n",
    "                    array_agg(item_id order by item_id) as all_items\n",
    "                from (\n",
    "                    select\n",
    "                        item_id\n",
    "                    from\n",
    "                        ml.fm_train_data\n",
    "                    group by 1\n",
    "                )t31\n",
    "            ) t3\n",
    "    ) distributed randomly;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_rows\n",
      "0      9430\n",
      "   num_cols\n",
      "0      2623\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"select count(*) as num_rows from ml.fm_test_table;\"\"\"\n",
    "print psql.read_sql(sql,conn)\n",
    "\n",
    "sql = \"\"\"select array_upper(x,1) as num_cols from ml.fm_test_table group by 1;\"\"\"\n",
    "print psql.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"drop table if exists ml.fm_test_predictions;\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()\n",
    "\n",
    "sql = \"\"\"\n",
    "    create table ml.fm_test_predictions as (\n",
    "        select\n",
    "            t1.id,\n",
    "            t1.y,\n",
    "            ml.fm_predict (\n",
    "                t1.x,\n",
    "                t2.n,\n",
    "                t2.k,\n",
    "                t2.w,\n",
    "                t2.miny,\n",
    "                t2.maxy\n",
    "            ) as y_hat\n",
    "        from\n",
    "            ml.fm_test_table t1,\n",
    "            ml.fm_mdl_table t2 \n",
    "    ) distributed randomly;\n",
    "\"\"\"\n",
    "psql.execute(sql,conn)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.062072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_squared_error\n",
       "0            1.062072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select\n",
    "        avg(se) as mean_squared_error\n",
    "    from (\n",
    "        select\n",
    "            (y_hat-y)^2 as se\n",
    "        from\n",
    "            ml.fm_test_predictions\n",
    "    )t\n",
    "\"\"\"\n",
    "psql.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
